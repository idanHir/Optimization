{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "osfFbOiTGXn0"
   },
   "source": [
    "## Imports and stting functions"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "cellView": "form",
    "id": "eKkkKJ0qDYb8",
    "ExecuteTime": {
     "end_time": "2025-12-28T17:23:29.066087Z",
     "start_time": "2025-12-28T17:23:29.056893Z"
    }
   },
   "source": [
    "# @title Imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import time"
   ],
   "outputs": [],
   "execution_count": 83
  },
  {
   "cell_type": "code",
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yft3XDWxDTPg",
    "outputId": "c752a4fa-86cc-48ca-ff64-76986bf89be6",
    "ExecuteTime": {
     "end_time": "2025-12-28T17:23:29.256026Z",
     "start_time": "2025-12-28T17:23:29.068042Z"
    }
   },
   "source": [
    "# @title Data Loader\n",
    "print(\"Loading Data...\")\n",
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
    "train_data = datasets.FashionMNIST(root='./data', train=True, transform=transform, download=True)\n",
    "x_train = train_data.data\n",
    "image_size = x_train.shape[1] * x_train.shape[2]\n",
    "x_train = x_train.reshape(-1, image_size).float() / 255"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Data...\n"
     ]
    }
   ],
   "execution_count": 84
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "ykXQg9hDDor7",
    "cellView": "form",
    "ExecuteTime": {
     "end_time": "2025-12-28T17:23:29.266779Z",
     "start_time": "2025-12-28T17:23:29.257030Z"
    }
   },
   "source": [
    "# @title AutoEncoder\n",
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(image_size, 256), nn.ReLU(),\n",
    "            nn.Linear(256, 64), nn.ReLU(),\n",
    "            nn.Linear(64, 32)\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(32, 64), nn.ReLU(),\n",
    "            nn.Linear(64, 128), nn.ReLU(),\n",
    "            nn.Linear(128, image_size), nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return decoded"
   ],
   "outputs": [],
   "execution_count": 85
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Q_ill-I0HBJb",
    "ExecuteTime": {
     "end_time": "2025-12-28T17:23:29.288099Z",
     "start_time": "2025-12-28T17:23:29.267786Z"
    }
   },
   "source": [
    "#init autoencoder\n",
    "autoencoder = Autoencoder()"
   ],
   "outputs": [],
   "execution_count": 86
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bakHXzqJa_eL"
   },
   "source": [
    "## oracle functions\n",
    "you should use to given model, loss function and x returns either the model output, the loss, the gradient and the hessian."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "-20Y4GO_ECuk",
    "ExecuteTime": {
     "end_time": "2025-12-28T17:23:29.300286Z",
     "start_time": "2025-12-28T17:23:29.291105Z"
    }
   },
   "source": [
    "def autoencoder_oracle(model, criterion, x, calc_hessian=False):\n",
    "    \"\"\"\n",
    "    Computes f(x), gradient, and Hessian for a given input x.\n",
    "    \"\"\"\n",
    "    # Forward Pass\n",
    "    reconstructed_x = model(x)\n",
    "    loss = criterion(reconstructed_x, x)\n",
    "\n",
    "    grads = torch.autograd.grad(loss, model.parameters(), create_graph=calc_hessian)\n",
    "\n",
    "    hessians = []\n",
    "\n",
    "    if calc_hessian:\n",
    "        for i, (grad, param) in enumerate(zip(grads, model.parameters())):\n",
    "\n",
    "            grad_flat = grad.view(-1)\n",
    "\n",
    "            hessian_rows = []\n",
    "            for j in range(len(grad_flat)):\n",
    "                grad_2nd = torch.autograd.grad(grad_flat[j], param, retain_graph=True)[0]\n",
    "                hessian_rows.append(grad_2nd.view(-1))\n",
    "\n",
    "            hessian_matrix = torch.stack(hessian_rows)\n",
    "            hessians.append(hessian_matrix)\n",
    "\n",
    "    return loss, grads, hessians"
   ],
   "outputs": [],
   "execution_count": 87
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Fu0KkS3fD4Nm",
    "outputId": "541d9926-650b-4d62-f988-f7a2f262fb01",
    "ExecuteTime": {
     "end_time": "2025-12-28T17:23:29.316020Z",
     "start_time": "2025-12-28T17:23:29.302800Z"
    }
   },
   "source": [
    "print(\"Initializing the model (Defining the function surface)...\")\n",
    "train_dataset = TensorDataset(x_train)\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "# This dataset is for the second order learners - so you wont run out of RAM\n",
    "small_train_dataset = TensorDataset(x_train[:100])  # רק 100 תמונות להדגמה\n",
    "small_train_loader = DataLoader(small_train_dataset, batch_size=10, shuffle=True)\n",
    "\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing the model (Defining the function surface)...\n"
     ]
    }
   ],
   "execution_count": 88
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iCrXpA4MgRLv"
   },
   "source": [
    "## Examples for optimizers"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Y3cN2BFTELxW",
    "ExecuteTime": {
     "end_time": "2025-12-28T17:23:29.322617Z",
     "start_time": "2025-12-28T17:23:29.317103Z"
    }
   },
   "source": [
    "def gd_step(x, grad_f_x, learning_rate=0.1):\n",
    "    \"\"\"\n",
    "    Performs one step of Gradient Descent.\n",
    "    Formula: x_new = x - lr * gradient\n",
    "    \"\"\"\n",
    "    x_new = x - learning_rate * grad_f_x\n",
    "\n",
    "    return x_new"
   ],
   "outputs": [],
   "execution_count": 89
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "f3wszF7YERNy",
    "ExecuteTime": {
     "end_time": "2025-12-28T17:23:29.332115Z",
     "start_time": "2025-12-28T17:23:29.324142Z"
    }
   },
   "source": [
    "def newton_step(x, grad_f_x, hessian_f_x, learning_rate=1.0):\n",
    "    \"\"\"\n",
    "    Performs one step of Newton-Raphson optimization on a batch.\n",
    "    Shapes:\n",
    "      x: (Batch, Dim)\n",
    "      grad_f_x: (Batch, Dim)\n",
    "      hessian_f_x: (Batch, Dim, Dim)\n",
    "    \"\"\"\n",
    "    # 1. Calculate Inverse Hessian\n",
    "    # torch.linalg.pinv handles batch dimensions automatically (Batch, D, D)\n",
    "    hessian_inv = torch.linalg.pinv(hessian_f_x)\n",
    "\n",
    "    # 2. Prepare Gradient for Batch Matrix Multiplication\n",
    "    # grad_f_x is (Batch, D). We need (Batch, D, 1) for matmul\n",
    "    grad_unsqueezed = grad_f_x.unsqueeze(2)\n",
    "\n",
    "    # 3. Compute Update Direction: H^-1 * Gradient\n",
    "    # (Batch, D, D) x (Batch, D, 1) -> (Batch, D, 1)\n",
    "    update_direction = torch.matmul(hessian_inv, grad_unsqueezed)\n",
    "\n",
    "    # 4. Remove extra dimension: (Batch, D, 1) -> (Batch, D)\n",
    "    update_direction = update_direction.squeeze(2)\n",
    "\n",
    "    # 5. Apply Update\n",
    "    x_new = x - learning_rate * update_direction\n",
    "\n",
    "    return x_new"
   ],
   "outputs": [],
   "execution_count": 90
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Our models"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-28T20:23:56.695164Z",
     "start_time": "2025-12-28T20:23:56.606468Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import math\n",
    "\n",
    "class AdamWState:\n",
    "    \"\"\"\n",
    "    Minimal state container for AdamW, using article notation:\n",
    "    alpha (α), beta1 (β1), beta2 (β2), epsilon (ε), lambda_ (λ), eta_t (η_t).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, param, alpha=1e-3, beta1=0.9, beta2=0.999,\n",
    "                 epsilon=1e-8, lambda_weight_decay=0.05):\n",
    "        self.alpha = alpha  # α\n",
    "        self.beta1 = beta1  # β1\n",
    "        self.beta2 = beta2  # β2\n",
    "        self.epsilon = epsilon  # ε\n",
    "        self.lambda_ = lambda_weight_decay  # λ\n",
    "\n",
    "        # Global step\n",
    "        self.t = 0\n",
    "\n",
    "        # Per-parameter first and second moments: m_t, v_t\n",
    "        self.m = torch.zeros_like(param)\n",
    "        self.v = torch.zeros_like(param)\n",
    "\n",
    "    def eta_t(self):\n",
    "        \"\"\"\n",
    "        η_t schedule multiplier (SetScheduleMultiplier(t) in the paper).\n",
    "        For now: constant 1.0 (we can later plug in cosine annealing, etc.).\n",
    "        \"\"\"\n",
    "        return 1.0\n",
    "\n",
    "\n",
    "def adamw_step(param_data, grad, state: AdamWState):\n",
    "    \"\"\"\n",
    "    Single AdamW update step, using the gradient tensors from the oracle.\n",
    "    Implements Algorithm 2 (Adam with decoupled weight decay, AdamW). [file:2]\n",
    "\n",
    "    θ_t = θ_{t-1} - η_t * ( α * m_hat_t / (sqrt(v_hat_t) + ε) + λ * θ_{t-1} )\n",
    "    \"\"\"\n",
    "    state.t += 1\n",
    "    t = state.t\n",
    "    beta1, beta2 = state.beta1, state.beta2\n",
    "    alpha = state.alpha\n",
    "    eps = state.epsilon\n",
    "    lam = state.lambda_\n",
    "    eta_t = state.eta_t()\n",
    "\n",
    "    # m_t = β1 m_{t-1} + (1-β1) g_t\n",
    "    state.m.mul_(beta1).add_(grad, alpha=(1.0 - beta1))\n",
    "\n",
    "    # v_t = β2 v_{t-1} + (1-β2) g_t^2\n",
    "    state.v.mul_(beta2).addcmul_(grad, grad, value=(1.0 - beta2))\n",
    "\n",
    "    # Bias correction\n",
    "    bias_correction1 = 1.0 - beta1 ** t\n",
    "    bias_correction2 = 1.0 - beta2 ** t\n",
    "    m_hat = state.m / bias_correction1\n",
    "    v_hat = state.v / bias_correction2\n",
    "\n",
    "    # Adam direction (loss-driven part)\n",
    "    adam_direction = m_hat / (v_hat.sqrt() + eps)\n",
    "\n",
    "    # θ_t = θ_{t-1} - η_t * ( α * adam_direction + λ * θ_{t-1} )\n",
    "    update = alpha * adam_direction + lam * param_data\n",
    "    return param_data - eta_t * update\n",
    "\n",
    "\n",
    "def rms(tensor: torch.Tensor):\n",
    "    return tensor.pow(2).mean().sqrt()\n",
    "\n",
    "\n",
    "class AdafactorState:\n",
    "    \"\"\"\n",
    "    Minimal state container for Adafactor (Algorithms 4 & 5). [file:1]\n",
    "\n",
    "    Uses:\n",
    "    - ρ_t schedule (relative step-size multiplier)\n",
    "    - β2_t schedule (second-moment decay)\n",
    "    - ε1, ε2, d as in the paper\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, param,\n",
    "                 epsilon1=1e-30, epsilon2=1e-3,\n",
    "                 d=1.0,\n",
    "                 beta2_exponent=0.8):\n",
    "        # Small constants\n",
    "        self.epsilon1 = epsilon1  # ε1 (for second-moment denom)\n",
    "        self.epsilon2 = epsilon2  # ε2 (for RMS lower bound)\n",
    "        self.d = d  # clipping threshold\n",
    "\n",
    "        # Schedules for β2_t, ρ_t; we use the paper's proposal: β2_t = 1 - t^{-c}. [file:1]\n",
    "        self.beta2_exponent = beta2_exponent\n",
    "\n",
    "        self.t = 0\n",
    "\n",
    "        # Factored second-moment states per parameter:\n",
    "        # For matrices: R_t (rows), C_t (cols)\n",
    "        # For vectors: V_t\n",
    "        if param.ndim == 2:\n",
    "            n, m = param.shape\n",
    "            self.is_matrix = True\n",
    "            self.R = torch.zeros(n, device=param.device, dtype=param.dtype)\n",
    "            self.C = torch.zeros(m, device=param.device, dtype=param.dtype)\n",
    "            self.V = None\n",
    "        else:\n",
    "            self.is_matrix = False\n",
    "            self.R = None\n",
    "            self.C = None\n",
    "            self.V = torch.zeros_like(param)\n",
    "\n",
    "    def beta2_t(self):\n",
    "        \"\"\"\n",
    "        β2_t schedule, as in section 7.2: β2_t = 1 - t^{-c}, with c = beta2_exponent. [file:1]\n",
    "        Ensures β2_1 = 0, increasing towards 1.\n",
    "        \"\"\"\n",
    "        t = max(self.t, 1)\n",
    "        c = self.beta2_exponent\n",
    "        return 1.0 - t ** (-c)\n",
    "\n",
    "    def rho_t(self):\n",
    "        \"\"\"\n",
    "        ρ_t (relative step-size scalar). Paper suggests schedules like:\n",
    "        ρ_t = min(lr_scale / sqrt(t), lr_scale / t). [file:1]\n",
    "\n",
    "        For now, use a simple schedule; we can tune it:\n",
    "        ρ_t = min(1e-2, 1.0 / sqrt(t))\n",
    "        \"\"\"\n",
    "        t = max(self.t, 1)\n",
    "        return min(1e-2, 1.0 / math.sqrt(t))\n",
    "\n",
    "\n",
    "def adafactor_step(param_data, grad, state: AdafactorState):\n",
    "    \"\"\"\n",
    "    Single Adafactor update for all parameters, using oracle gradients. [file:1]\n",
    "\n",
    "    For each parameter X:\n",
    "      1. α_t = max(ε2, RMS(X_{t-1})) * ρ_t\n",
    "      2. Update second-moment estimates (factored for matrices, vector V_t for 1D)\n",
    "      3. U_t = G_t / sqrt(V_t + ε1)\n",
    "      4. Clip U_t by RMS if needed\n",
    "      5. X_t = X_{t-1} - α_t * U_t\n",
    "      \n",
    "    Per-parameter Adafactor update. Returns updated param_data.\n",
    "    \"\"\"\n",
    "    state.t += 1\n",
    "    beta2_t = state.beta2_t()\n",
    "    rho_t = state.rho_t()\n",
    "    eps1 = state.epsilon1\n",
    "    eps2 = state.epsilon2\n",
    "    d = state.d\n",
    "\n",
    "    # Step-size scale from parameter RMS\n",
    "    rms_x = max(eps2, param_data.pow(2).mean().sqrt().item())\n",
    "    alpha_t = rms_x * rho_t  # α_t = max(ε2, RMS(X_{t-1})) * ρ_t\n",
    "    \n",
    "    G = grad\n",
    "    \n",
    "    # Row and column means of squared gradients\n",
    "    grad_sq = G.pow(2)\n",
    "\n",
    "    if state.is_matrix:\n",
    "        # Matrix case, Algorithm 4. [file:1]\n",
    "\n",
    "        # Row means: (n,)\n",
    "        row_mean = grad_sq.mean(dim=1)\n",
    "        # Column means: (m,)\n",
    "        col_mean = grad_sq.mean(dim=0)\n",
    "\n",
    "        # R_t update\n",
    "        state.R.mul_(beta2_t).add_(row_mean, alpha=(1.0 - beta2_t))\n",
    "        # C_t update\n",
    "        state.C.mul_(beta2_t).add_(col_mean, alpha=(1.0 - beta2_t))\n",
    "\n",
    "        # Reconstruct V_t: outer product, normalized by mean(R_t). [file:1]\n",
    "        R_t = state.R\n",
    "        C_t = state.C\n",
    "        # Outer product (n x m)\n",
    "        V_t = torch.ger(R_t, C_t)  # R_t[:, None] * C_t[None, :]\n",
    "        denom = R_t.mean()\n",
    "        if denom > 0:\n",
    "            V_t = V_t / denom\n",
    "        else:\n",
    "            V_t = V_t + eps1  # safeguard\n",
    "\n",
    "    else:\n",
    "        # Vector or fallback case, Algorithm 5. [file:1]\n",
    "        \n",
    "        # Use mean over all elements\n",
    "        mean_grad_sq = grad_sq.mean()\n",
    "        state.V.mul_(beta2_t).add_(mean_grad_sq, alpha=(1.0 - beta2_t))\n",
    "        V_t = state.V.expand_as(G)\n",
    "\n",
    "    # Unscaled update\n",
    "    U_t = G / (V_t.sqrt() + eps1)\n",
    "\n",
    "    # Update clipping by RMS(U_t)\n",
    "    rms_u = U_t.pow(2).mean().sqrt().item()\n",
    "    clip_denom = max(1.0, rms_u / d)\n",
    "    U_t = U_t / clip_denom\n",
    "\n",
    "    return param_data - alpha_t * U_t\n",
    "\n",
    "# -------------------------\n",
    "# Cosine schedule with warm restarts (AdamWR-style, to be used for Adafactor++)\n",
    "# -------------------------\n",
    "\n",
    "class CosineWarmRestartSchedule:\n",
    "    \"\"\"\n",
    "    Cosine multiplier η_t with warm restarts, as in SGDR/AdamWR. [file:2]\n",
    "\n",
    "    Tracks:\n",
    "    - T_i: current cycle length (in \"epochs\" or effective units you choose)\n",
    "    - T_cur: progress inside current cycle\n",
    "    You can call .step(delta) each epoch (or every k steps) to advance T_cur.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, Ti_initial=5.0, T_mult=2.0, eta_min=0.0, eta_max=1.0):\n",
    "        self.Ti = Ti_initial      # current cycle length\n",
    "        self.T_mult = T_mult\n",
    "        self.eta_min = eta_min\n",
    "        self.eta_max = eta_max\n",
    "        self.T_cur = 0.0\n",
    "\n",
    "    def step_epoch(self, delta_epochs=1.0):\n",
    "        \"\"\"\n",
    "        Call this once per epoch (or scaled appropriately) to advance the schedule.\n",
    "        \"\"\"\n",
    "        self.T_cur += delta_epochs\n",
    "        if self.T_cur >= self.Ti:\n",
    "            # restart\n",
    "            self.T_cur = 0.0\n",
    "            self.Ti *= self.T_mult\n",
    "\n",
    "    def eta_t(self):\n",
    "        \"\"\"\n",
    "        Cosine multiplier in [eta_min, eta_max]. [file:2]\n",
    "        \"\"\"\n",
    "        cos_inner = math.pi * self.T_cur / self.Ti\n",
    "        cos_val = math.cos(cos_inner)\n",
    "        # return self.eta_min + 0.5 * (self.eta_max - self.eta_min) * (1.0 + cos_val)\n",
    "        return 1\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# Adafactor-W-WR per-parameter state\n",
    "# -------------------------\n",
    "\n",
    "class AdafactorWWRState:\n",
    "    \"\"\"\n",
    "    Per-parameter state for improved Adafactor:\n",
    "    - Factored second moments (R, C) for matrices; V for vectors. [file:1]\n",
    "    - Decoupled weight decay (λ). [file:2]\n",
    "    - Relative step size with RMS scaling. [file:1]\n",
    "    \"\"\"\n",
    "    def __init__(self, param,\n",
    "                 epsilon1=1e-30, epsilon2=1e-3,\n",
    "                 d=1.0, beta2_exponent=0.8,\n",
    "                 rho_max=1e-2,\n",
    "                 lambda_norm=0.0,\n",
    "                 batch_size=None, dataset_size=None, total_epochs=None,\n",
    "                 schedule: CosineWarmRestartSchedule = None):\n",
    "        self.epsilon1 = epsilon1   # ε1\n",
    "        self.epsilon2 = epsilon2   # ε2\n",
    "        self.d = d                # clipping threshold\n",
    "        self.beta2_exponent = beta2_exponent  # c in β2_t = 1 - t^{-c} [file:1]\n",
    "        self.rho_max = rho_max    # cap for ρ_t\n",
    "        self.t = 0\n",
    "\n",
    "        # Factored second-moment storage\n",
    "        if param.ndim == 2:\n",
    "            n, m = param.shape\n",
    "            self.is_matrix = True\n",
    "            self.R = torch.zeros(n, device=param.device, dtype=param.dtype)\n",
    "            self.C = torch.zeros(m, device=param.device, dtype=param.dtype)\n",
    "            self.V = None\n",
    "        else:\n",
    "            self.is_matrix = False\n",
    "            self.R = None\n",
    "            self.C = None\n",
    "            self.V = torch.zeros_like(param)\n",
    "\n",
    "        # Cosine warm restart schedule (shared or per-param)\n",
    "        self.schedule = schedule if schedule is not None else CosineWarmRestartSchedule()\n",
    "\n",
    "        # Normalized weight decay λ from λ_norm. [file:2]\n",
    "        if lambda_norm > 0.0 and batch_size is not None and dataset_size is not None and total_epochs is not None:\n",
    "            self.lambda_ = lambda_norm * (batch_size / (dataset_size * total_epochs))\n",
    "        else:\n",
    "            self.lambda_ = 0.0\n",
    "\n",
    "    def beta2_t(self):\n",
    "        t = max(self.t, 1)\n",
    "        c = self.beta2_exponent\n",
    "        return 1.0 - t ** (-c)\n",
    "\n",
    "    def rho_t(self):\n",
    "        t = max(self.t, 1)\n",
    "        return min(self.rho_max, 1.0 / math.sqrt(t))\n",
    "\n",
    "    def eta_t(self):\n",
    "        # Cosine multiplier η_t. [file:2]\n",
    "        return self.schedule.eta_t()\n",
    "\n",
    "# -------------------------\n",
    "# Adafactor-W-WR per-parameter step\n",
    "# -------------------------\n",
    "\n",
    "def adafactor_wwr_step(param_data, grad, state: AdafactorWWRState):\n",
    "    \"\"\"\n",
    "    Per-parameter Adafactor with:\n",
    "    - factored second moments & relative step size (Adafactor) [file:1]\n",
    "    - decoupled normalized weight decay (AdamW) [file:2]\n",
    "    - cosine warm restarts multiplier (AdamWR) [file:2]\n",
    "\n",
    "    Returns updated param_data.\n",
    "    \"\"\"\n",
    "    state.t += 1\n",
    "    beta2_t = state.beta2_t()\n",
    "    rho_t = state.rho_t()\n",
    "    eps1 = state.epsilon1\n",
    "    eps2 = state.epsilon2\n",
    "    d = state.d\n",
    "    lam = state.lambda_\n",
    "    \n",
    "    # advance the schedule multipiler\n",
    "    state.schedule.step_epoch(1.0)\n",
    "    \n",
    "    # set the current eta\n",
    "    eta_t = state.eta_t()\n",
    "    \n",
    "    # α_t = η_t * max(ε2, RMS(X_t)) * ρ_t\n",
    "    rms_x = max(eps2, param_data.pow(2).mean().sqrt().item())\n",
    "    alpha_t = eta_t * rms_x * rho_t\n",
    "\n",
    "    G = grad\n",
    "    grad_sq = G.pow(2)\n",
    "\n",
    "    # --- update second-moment stats (factored or vector) ---\n",
    "    if state.is_matrix:\n",
    "        # Matrix case: row/column stats [file:1]\n",
    "        row_mean = grad_sq.mean(dim=1)  # (n,)\n",
    "        col_mean = grad_sq.mean(dim=0)  # (m,)\n",
    "\n",
    "        state.R.mul_(beta2_t).add_(row_mean, alpha=(1.0 - beta2_t))\n",
    "        state.C.mul_(beta2_t).add_(col_mean, alpha=(1.0 - beta2_t))\n",
    "\n",
    "        R_t = state.R\n",
    "        C_t = state.C\n",
    "        V_t = torch.ger(R_t, C_t)\n",
    "        denom = R_t.mean()\n",
    "        if denom > 0:\n",
    "            V_t = V_t / denom\n",
    "        else:\n",
    "            V_t = V_t + eps1\n",
    "    else:\n",
    "        # Vector / fallback: single V_t [file:1]\n",
    "        mean_grad_sq = grad_sq.mean()\n",
    "        state.V.mul_(beta2_t).add_(mean_grad_sq, alpha=(1.0 - beta2_t))\n",
    "        V_t = state.V.expand_as(G)\n",
    "\n",
    "    # --- adaptive direction ---\n",
    "    U_t = G / (V_t.sqrt() + eps1)\n",
    "\n",
    "    # Update clipping\n",
    "    rms_u = U_t.pow(2).mean().sqrt().item()\n",
    "    clip_denom = max(1.0, rms_u / d)\n",
    "    U_t = U_t / clip_denom\n",
    "\n",
    "    # --- decoupled weight decay (AdamW-style) ---\n",
    "    if lam != 0.0:\n",
    "        param_data = param_data - alpha_t * lam * param_data\n",
    "\n",
    "    # --- final Adafactor update ---\n",
    "    param_data = param_data - alpha_t * U_t\n",
    "\n",
    "    return param_data\n"
   ],
   "outputs": [],
   "execution_count": 149
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "f3af72ae",
    "ExecuteTime": {
     "end_time": "2025-12-28T20:23:56.727936Z",
     "start_time": "2025-12-28T20:23:56.706113Z"
    }
   },
   "source": [
    "def run_model_optimization_experiment(\n",
    "        optimizer_type: str,\n",
    "        model: nn.Module,\n",
    "        criterion: nn.Module,\n",
    "        train_loader: DataLoader,\n",
    "        epochs: int,\n",
    "        learning_rate: float\n",
    "):\n",
    "    \"\"\"\n",
    "    Runs an optimization experiment to train the MODEL using GD or Newton.\n",
    "    Closer to the requested structure.\n",
    "    \"\"\"\n",
    "    all_losses = []\n",
    "    print(f\"\\nStarting MODEL {optimizer_type.upper()} optimization for {epochs} epochs (lr={learning_rate})...\")\n",
    "    total_start_time = time.time()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        epoch_loss = 0\n",
    "\n",
    "        for batch_idx, batch in enumerate(train_loader):\n",
    "            img = batch[0]\n",
    "\n",
    "            calc_hessian_for_oracle = (optimizer_type == 'newton')\n",
    "            loss, grads, hessians = autoencoder_oracle(model, criterion, img, calc_hessian=calc_hessian_for_oracle)\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "            # 2. Optimization step\n",
    "            with torch.no_grad():\n",
    "                if optimizer_type == 'gd':\n",
    "                    for param, grad in zip(model.parameters(), grads):\n",
    "                        param.data = gd_step(param.data, grad, learning_rate)\n",
    "\n",
    "                elif optimizer_type == 'newton':\n",
    "                    if hessians is None:\n",
    "                        raise ValueError(\"Hessian missing for Newton method\")\n",
    "                    for param, grad, hessian in zip(model.parameters(), grads, hessians):\n",
    "                        param.data = newton_step(param.data, grad, hessian, learning_rate)\n",
    "\n",
    "                elif optimizer_type == 'adamw':\n",
    "                    for (param, grad, state) in zip(model.parameters(), grads, adamw_states):\n",
    "                        param.data = adamw_step(param.data, grad, state)\n",
    "\n",
    "                elif optimizer_type == 'adafactor':\n",
    "                    for (param, grad, state) in zip(model.parameters(), grads, adafactor_states):\n",
    "                        param.data = adafactor_step(param.data, grad, state)\n",
    "                \n",
    "                elif optimizer_type == 'adafactor_wwr':\n",
    "                    for (param, grad, state) in zip(model.parameters(), grads, adafactor_wwr_states):\n",
    "                        param.data = adafactor_wwr_step(param.data, grad, state)\n",
    "\n",
    "                else:\n",
    "                    raise ValueError(\"Invalid optimizer type\")\n",
    "\n",
    "        avg_loss = epoch_loss / len(train_loader)\n",
    "        all_losses.append(avg_loss)\n",
    "\n",
    "        if (epoch + 1) % (epochs // 5) == 0 or epoch == 0 or epoch == epochs - 1:\n",
    "            print(f\"  Epoch [{epoch + 1}/{epochs}] Loss: {avg_loss:.6f}\")\n",
    "\n",
    "    total_end_time = time.time()\n",
    "    total_training_time = total_end_time - total_start_time\n",
    "    print(f\"\\nTotal Training Time: {total_training_time:.2f} seconds\")\n",
    "    print(f\"Finished {optimizer_type.upper()} optimization. Final Loss: {all_losses[-1]:.6f}\")\n",
    "    return model, all_losses"
   ],
   "outputs": [],
   "execution_count": 150
  },
  {
   "cell_type": "code",
   "source": [
    "epochs = 50\n",
    "lr = 0.01\n",
    "model = Autoencoder()\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# Storing the hyperparameters in a dedicated state of each algorithm\n",
    "adamw_states = [\n",
    "    AdamWState(p.data, alpha=lr, beta1=0.9, beta2=0.999,\n",
    "               epsilon=1e-8, lambda_weight_decay=0.01)\n",
    "    for p in model.parameters()\n",
    "]\n",
    "\n",
    "adafactor_states = [\n",
    "    AdafactorState(p.data,\n",
    "                   epsilon1=1e-30, epsilon2=1e-3,\n",
    "                   d=1.0, beta2_exponent=0.8)\n",
    "    for p in model.parameters()\n",
    "]\n",
    "\n",
    "shared_schedule = CosineWarmRestartSchedule(Ti_initial=5.0, T_mult=3.0)\n",
    "\n",
    "adafactor_wwr_states = [\n",
    "    AdafactorWWRState(\n",
    "        p.data,\n",
    "        epsilon1=1e-30,\n",
    "        epsilon2=1e-3,\n",
    "        d=1.0,\n",
    "        beta2_exponent=0.8,\n",
    "        rho_max=1e-2,\n",
    "        lambda_norm=3e-2,\n",
    "        batch_size=train_loader.batch_size,\n",
    "        dataset_size=len(train_loader.dataset),\n",
    "        total_epochs=epochs,\n",
    "        schedule=shared_schedule\n",
    "    )\n",
    "    for p in model.parameters()\n",
    "]\n",
    "\n",
    "\n",
    "trained_model, losses = run_model_optimization_experiment(\n",
    "    'adafactor_wwr', model, criterion, train_loader, epochs, lr\n",
    ")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tLFlHffz_pW7",
    "outputId": "eaee5f89-48b6-4a9f-d9c6-a7b3bde538dc",
    "ExecuteTime": {
     "end_time": "2025-12-28T20:35:10.079125Z",
     "start_time": "2025-12-28T20:33:33.557225Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting MODEL ADAFACTOR_WWR optimization for 100 epochs (lr=0.01)...\n",
      "  Epoch [1/100] Loss: 0.041634\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "execution_count": 154
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "12399dc4"
   },
   "source": [
    "### Loss Curves Over Epochs"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "13cac4f5",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 564
    },
    "outputId": "ebaa8145-7456-43e3-99d4-02a9423b8a83",
    "ExecuteTime": {
     "end_time": "2025-12-28T20:35:10.079125Z",
     "start_time": "2025-12-28T20:35:10.079125Z"
    }
   },
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(losses, label=f'GD (LR={lr})')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Optimization Loss Over Epochs')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "475df8df"
   },
   "source": [
    "### Visual Comparison of Optimized Images"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "bf4b5052",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "outputId": "72368616-1a79-4cec-dcda-efdfde5cb0c9"
   },
   "source": [
    "def show_model_reconstructions(model, batch_images, count=5):\n",
    "    \"\"\"\n",
    "    Visualizes original images vs. model reconstructions.\n",
    "    \"\"\"\n",
    "    print(\"\\nVisualizing Results...\")\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        inputs = batch_images[:count]\n",
    "        reconstructions = model(inputs)\n",
    "\n",
    "    plt.figure(figsize=(8, 3 * count))\n",
    "\n",
    "    for i in range(count):\n",
    "        img_orig = inputs[i].cpu().numpy().reshape(28, 28)\n",
    "        img_recon = reconstructions[i].cpu().numpy().reshape(28, 28)\n",
    "\n",
    "        # Original\n",
    "        ax = plt.subplot(count, 2, i * 2 + 1)\n",
    "        plt.imshow(img_orig, cmap='gray')\n",
    "        if i == 0: ax.set_title(\"Original\")\n",
    "        plt.axis('off')\n",
    "\n",
    "        #Reconstructed\n",
    "        ax = plt.subplot(count, 2, i * 2 + 2)\n",
    "        plt.imshow(img_recon, cmap='gray')\n",
    "        if i == 0: ax.set_title(\"Reconstructed (Model Output)\")\n",
    "        plt.axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# ---use example ---\n",
    "data_iter = iter(train_loader)\n",
    "sample_images = next(data_iter)[0]\n",
    "\n",
    "show_model_reconstructions(trained_model, sample_images, count=5)"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "name": "python3",
   "language": "python"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
